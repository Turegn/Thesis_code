{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages for the project\n",
    "import copy,csv,time,nltk,emoji,gensim,string \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from tabulate import tabulate  \n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords  \n",
    "import matplotlib.pyplot as plt \n",
    "from gensim.models import word2vec \n",
    "import gensim.models.keyedvectors as word2vec #need to use due to depreceated model\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras import models,layers  \n",
    "from keras.layers import Dense,Dropout,LSTM,Activation,Bidirectional,Flatten,Embedding\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer,one_hot,text_to_word_sequence\n",
    "from keras.initializers import Constant \n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import classification_report,precision_score,recall_score,f1_score,roc_auc_score,cohen_kappa_score,accuracy_score,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split,KFold,cross_val_score \n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.datasets import make_circles\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow import set_random_seed\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer\n",
    "from nltk import bigrams, trigrams\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import warnings\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "warnings.filterwarnings(\"ignore\")  #Ignoring unnecessory warnings\n",
    "# import the dataset to our project \n",
    "df=pd.read_csv('commentP_consN.csv',delimiter=';',names=['comment'],encoding=\"utf8\")\n",
    "all=pd.DataFrame(df) #changing to data frame \n",
    "print(all) #display the first n rows of comments\n",
    "print(all.info()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describition before preprocessing \n",
    "reviews_len = [len(x) for x in df['comment']]\n",
    "pd.Series(reviews_len).hist()\n",
    "print(plt.show())\n",
    "print(pd.Series(reviews_len).describe())\n",
    "# remove the comment to identify sum of words and maximum word before preprocessing \n",
    "'''\n",
    "df['tokenized_sents']= df.apply(lambda row: nltk.word_tokenize(row['comment']), axis=1)\n",
    "df['sents_length'] = df.apply(lambda row: len(row['tokenized_sents']),axis=1)\n",
    "print(df[['tokenized_sents','sents_length']])\n",
    "print(\"sum of words is\",sum(df['sents_length']))\n",
    "print(\"the maximum sents and minimum length is\",df['sents_length'].max(),df['sents_length'].min())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove digits \n",
    "df['comment'] = df['comment'].str.replace('\\d+', '')\n",
    "#print(df[['comment']])\n",
    "#the following is normalization of words and characters\n",
    "df['comment'] = df['comment'].replace(regex={\"ዐ\":\"አ\",\"ዑ\":\"ኡ\",\"ዒ\":\"ኢ\",\"ዓ\":\"አ\",\"ዔ\":\"ኤ\",\"ዕ\":\"እ\",\"ዖ\":\"ኦ\",\n",
    "                                            \"ዓ\":\"አ\",\"ኽ\":\"ህ\",\"ሃ\":\"ሀ\",\"ዉ\":\"ው\",\n",
    "                                            \"ጸ\":\"ፀ\",\"ጹ\":\"ፁ\",\"ጺ\":\"ፂ\",\"ጻ\":\"ፃ\",\"ጼ\":\"ፄ\",\"ጽ\":\"ፅ\",\"ጾ\":\"ፆ\",\n",
    "                                             \"ኃ\":\"ሀ\",\"ኁ\":\"ሁ\",\"ኂ\":\"ሂ\",\"ኃ\":\"ሀ\",\"ኄ\":\"ሄ\",\"ኅ\":\"ህ\",\"ኆ\":\"ሆ\",\n",
    "                                             \"ሠ\":\"ሰ\",\"ሡ\":\"ሱ\",\"ሢ\":\"ሲ\",\"ሣ\":\"ሳ\",\"ሤ\":\"ሴ\",\"ሥ\":\"ስ\",\"ሦ\":\"ሶ\",\n",
    "                                            \"ሐ\":\"ሀ\",\"ሑ\":\"ሁ\",\"ሒ\":\"ሂ\",\"ሓ\":\"ሀ\",\"ሔ\":\"ሄ\",\"ሕ\":\"ህ\",\"ሖ\":\"ሆ\",                                             \n",
    "                                             \"ኮመንት\":\"አስተያየት\",\"ሙቭ\":\"ፊልም\",\"ኢትዮጽያ\":\"ኢትዮጵያ\",\"ኢትዬ\":\"ኢትዮጵያ\",\n",
    "                                             \"ኢቲዮፒያ\":\"ኢትዮጵያ\",\"ፍልም\":\"ፊልም\",\"በጠም\":\"በጣም\",\"1ኛ\":\"አንደኛ\",\"ይመችክ\":\"ይመችህ\",\n",
    "                                             \"ይሁንላቹ\":\"ይሁንላችሁ\",\"አረ\":\"እረ\",\"ላይክ\":\"ዉደድ\",\"ፍቅርርር\":\"ፍቅር\",\"ፍቅርር\":\"ፍቅር\",\n",
    "                                             \"ሀሪፍ\":\"አሪፍ\",\"ሀሪፈ\":\"አሪፍ\",\"ሰውደወ\":\"ስወደው\",\"ዋውውውው\":\"ዋው\",\"ኡፍፍፍፍፍፍፍ\":\"ኡፍፍ\",\"ኡፍፍፍፍፍፍ\":\"ኡፍፍ\",\n",
    "                                             \"ኡፍፍፍፍፍ\":\"ኡፍፍ\",\"ኡፍፍፍ\":\"ኡፍ\",\"ውድድድድድ\":\"ውድድ\",\n",
    "                                            \"ውድድድድ\":\"ውድድ\",\"ውድድድ\":\"ውድድ\",\"1ኛ\":\"አንደኛ\",\"ኢትዩቢያዊ\":\"ኢትዮጽያዊ\"})\n",
    "print(df[['comment']])\n",
    "#removing htmls from each comment \n",
    "df['comment']=df['comment'].replace('{html}',\"\") \n",
    "freq = pd.Series(' '.join(df['comment']).split()).value_counts()[-10:]\n",
    "# uncomment the next line of code when Emojis are not considered\n",
    "df['comment'] = [''.join(c for c in s if c not in string.punctuation) for s in df['comment']]\n",
    "# uncomment the following line of codes when considering Emojis for comment labeling \n",
    "''' \n",
    "#df['comment'] = df['comment'].str.replace('[^\\w\\s]','') # for removing Emojis from the dataset uncomment it.\n",
    "for index, row in df.iterrows():\n",
    "     row=df['comment']==\"\"\n",
    "     index_no=df.index[row] #display index numbers(rows) that have element(values) \n",
    "aa=df['comment'].drop(index_no,axis=0)\n",
    "aa.to_csv('output.csv', index=False, header=None)\n",
    "df=pd.read_csv('output.csv',delimiter=';',names=['comment'],encoding=\"utf-8\")\n",
    "afteremojiR=pd.DataFrame(df) #changing to data frame\n",
    "print(afteremojiR)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing of the comments \n",
    "df['tokenized_sents']= df.apply(lambda row: nltk.word_tokenize(row['comment']), axis=1)\n",
    "# removing na values(containing empty values)\n",
    "df['tokenized_sents'].replace('', np.nan, inplace=True)\n",
    "#print(tabulate(dft['tokenized_sents']))\n",
    "frequency=df['tokenized_sents']\n",
    "# Removing Amharic Stopwords\n",
    "stop = stopwords.words('Amharic')\n",
    "df['stopwords'] = df['tokenized_sents'].apply(lambda x: len([item for item in x if item in stop]))\n",
    "df['tokenized_sents']=df['tokenized_sents'].apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sents_length'] = df.apply(lambda row: len(row['tokenized_sents']),axis=1)\n",
    "print(df[['tokenized_sents','sents_length']])\n",
    "print(\"sum of words is\",sum(df['sents_length']))\n",
    "print(\"the maximum sents length is\",df['sents_length'].sum())\n",
    "df4=df['tokenized_sents']\n",
    "df5=pd.Series.to_frame(df4) #to change series to frame'\n",
    "df5.columns=['tokenized_sents']\n",
    "#dfp=pd.read_csv('positive_emoji.csv',delimiter=';',names=['positive_emoji'],encoding=\"utf8\")\n",
    "#dfn=pd.read_csv('negative_emoji.csv',delimiter=';',names=['negative_emoji'],encoding=\"utf8\")\n",
    "df2=pd.read_csv('Negative.csv',names=['negative'],encoding=\"utf8\")\n",
    "df7=pd.read_csv('positive.csv',names=['positive'],encoding=\"utf8\")\n",
    "df8=pd.read_csv('Invertor.csv',names=['Invertor'],encoding=\"utf8\")\n",
    "df9=pd.read_csv('intensifierA.csv',names=['intensifier'],encoding=\"utf8\")\n",
    "columen_names=['negative','positive','Invertor','intensifierA']\n",
    "result=pd.concat([df2,df7,df8,df9],axis=1,join='outer',sort=False,names=columen_names)\n",
    "blankIndex=[''] * len(result)\n",
    "result.index=blankIndex\n",
    "#information about lexicons \n",
    "print(\"number of positive lexicon\",df8.info())\n",
    "print(\"number of negative lexicon\",df9.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_count=0\n",
    "negative_count=0\n",
    "invertor_count=0\n",
    "intensifier_count=0\n",
    "pos_emoji=0\n",
    "positive_emoji_count=0\n",
    "negative_emoji_count=0\n",
    "emoji_score=0\n",
    "#review length analysis used to include or exclude very short or lengthy statements\n",
    "reviews_len = [len(x) for x in df['comment']]\n",
    "pd.Series(reviews_len).hist()\n",
    "print(plt.show())\n",
    "print(pd.Series(reviews_len).describe())\n",
    "df10 = pd.DataFrame(columns=['polarity'])\n",
    "# The following are list of labeled positive and negative Emojis\n",
    "def extract_emoji(str):\n",
    "            return ''.join(c for c in str if c in emoji.UNICODE_EMOJI)\n",
    "positive_emoji=['😂','❤','♥','😍','😘','😊','👌','💕','👏','😁','☺','♡','👍','🙏','✌','😏',\n",
    "                '😉','🙌','🙈','√', '❤️','♥️','♥️','❤️',\n",
    "'💪','😄','😃','😱','🎉','😜','🌸','💜','💙','😳','💗','☀','😎','😢','💋','😋','🙊','🎶','💞','💝'\n",
    "'😌','💯','💛','💁','💚','😆','😝','😅','👊','😀','😚','😻','💘','👋','✋','🎊','❄','😥','😈',\n",
    "'🔝','😰','⚽','👑','😹','🍃','🎁','🐧','🎈','✊','💤','💓','💦','🙋','🎄','🎵','😛','😬','👯',\n",
    "'💎','🎂','👫','🏆','☝','😙','⛄','👅','♪','🍂','💏','🌴','👈','🌹','🙆','💰','🍻''🌞','🍁','⭐',\n",
    "'🎀','🙉','💅','🌺','🐶','🌚','🎤','👭','🎧','👆','🍸','😇','🍉','🎸','🍹','💫','📚','🌷','💝',\n",
    "'💨','🏈','💍','☔','👸','🇪','😸','🍔','👼','🐯','😵','👶','💐','↾','📖','🐒','🌍','┊','🐥','💄',\n",
    "'💸','⛔','🏀','💉','💟','😯','♦','🌙','🐟','👣','🗿','🍝','🍭','❌','🐰','💊','🚨','🍪','✧','🎆',\n",
    "'🎎','🇩','✅','🔊','👠','🌌','🍎','🐻','💇','🍊','🍒','🐭','👟','🌎','🍍','🐮','📲','🌅','🇷','🌽',\n",
    "'🍬','😺','🚀','¦🍧','🍜','🆗','🍋','➤','🏄','👧','🐏','✏','🌾','🏡','👙','⛅','🍅','📺','🐍','🇮',\n",
    "'♣🇹','🐬','🌳','💿','🔐','🐨','🌕','🔵','🍳','🚴','👰','⚓','👗','➕','💬','🔜','🍨','🍙','🍗','🍲',\n",
    "'😼','🐙','👨','🍚','🍖','♨▃','🚘']\n",
    "negative_emoji=['😭',' 😩','😒','💖','😔','😡','😴','🔫','😞','😪',' 😫','💀','😕',\n",
    "'💔','😤','😑','😠','😓','😣','😲','😿','😐' '😨','😷','👎','💩','🙅','😶','🔪','💃',\n",
    "                '👿','✂','👪','😦','🍣','🙍','💧','😾','🍥','😭'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rules for labeling comments using labeled lexicons and Emoji\n",
    "for (index_label, row_series) in df5.iterrows():\n",
    "    list=row_series.values  #accessing the row values\n",
    "    for i in range(len(list)):\n",
    "         #print(list[i])\n",
    "        ll=len(list[i])\n",
    "        #print(ll)\n",
    "        li=list[i]\n",
    "        print(li)              #if df2['negative'].isin(li[j]) \n",
    "        xx=extract_emoji(li)\n",
    "        #print(xx)    \n",
    "        y=[]\n",
    "        for s in xx:\n",
    "            y.append(s)\n",
    "            ''.join(y)\n",
    "        #print(y)\n",
    "        pos_emojiexist=[]\n",
    "        neg_emojiexist=[]\n",
    "        for element in positive_emoji:\n",
    "            for word in y:\n",
    "                pos_emojiexist.append(word in element)\n",
    "                positive_emoji_count=sum(pos_emojiexist)\n",
    "        for item in negative_emoji:\n",
    "            for word in y:\n",
    "                neg_emojiexist.append(word in item)\n",
    "                negative_emoji_count=sum(neg_emojiexist)\n",
    "        if len(y)!=0:\n",
    "            emoji_score=positive_emoji_count-negative_emoji_count\n",
    "        else:\n",
    "            emoji_score=0\n",
    "        positive_res=result.positive.isin(li)\n",
    "        negative_res=result.negative.isin(li)\n",
    "        invertor_res=result.Invertor.isin(li)\n",
    "        intensifierA_res=result.intensifier.isin(li)\n",
    "        positive_count=sum(positive_res)*2\n",
    "        negative_count=sum(negative_res)*2\n",
    "        invertor_count=sum(invertor_res)\n",
    "        intensifier_count=sum(intensifierA_res)\n",
    "        sentiment_score=positive_count-negative_count\n",
    "        if invertor_count>0 and intensifier_count==0 and sentiment_score>0 and  ll>0:\n",
    "            sentiment=-(sentiment_score)\n",
    "        if invertor_count>0 and intensifier_count==0 and sentiment_score<0  and ll>0:\n",
    "            sentiment=-(sentiment_score)\n",
    "        if intensifier_count>0 and invertor_count==0 and sentiment_score>0  and ll>0:\n",
    "            sentiment=sentiment_score+3\n",
    "        if intensifier_count> 0 and invertor_count==0 and sentiment_score <0 and ll>0:\n",
    "            sentiment=sentiment_score-3\n",
    "        if invertor_count==0 and intensifier_count==0 and sentiment_score >0 and ll>0:\n",
    "            sentiment=sentiment_score\n",
    "        if invertor_count==0 and intensifier_count==0 and sentiment_score <0 and   ll>0:\n",
    "            sentiment=sentiment_score\n",
    "        if intensifier_count> 0 and invertor_count> 0 and sentiment_score>0  and ll>0:\n",
    "            sentiment=-(sentiment_score)+3\n",
    "        if intensifier_count> 0 and invertor_count> 0 and sentiment_score<0 and ll>0:\n",
    "            sentiment=-(sentiment_score)-3\n",
    "        if intensifier_count> 0 and invertor_count==0 and sentiment_score==0 and ll>0:\n",
    "             sentiment=sentiment_score\n",
    "        if sentiment_score==0 and invertor_count==0 and intensifier_count==0 and ll>0:\n",
    "             sentiment=sentiment_score\n",
    "        if sentiment>0 and emoji_score>=0:\n",
    "            print(\"positive\")\n",
    "            df10 = df10.append({'polarity': \"positive\"}, ignore_index=True)\n",
    "        if sentiment==0 and emoji_score>0:\n",
    "            print(\"positive\")\n",
    "            df10 = df10.append({'polarity': \"positive\"}, ignore_index=True)\n",
    "        if sentiment<0 and emoji_score<=0:\n",
    "            print(\"negative\")\n",
    "            df10 = df10.append({'polarity': \"negative\"}, ignore_index=True)\n",
    "        if sentiment==0 and emoji_score<0:\n",
    "            print(\"negative\")\n",
    "            df10 = df10.append({'polarity': \"negative\"}, ignore_index=True)\n",
    "        if sentiment<0 and emoji_score>0:#no change word first\n",
    "            print(\"negative\")\n",
    "            df10 = df10.append({'polarity': \"negative\"}, ignore_index=True)\n",
    "        if sentiment > 0 and emoji_score<0: #word first\n",
    "            print(\"positive\")\n",
    "            df10 = df10.append({'polarity': \"positive\"}, ignore_index=True)\n",
    "        if sentiment==0 and emoji_score==0:\n",
    "            print(\"neutral\")\n",
    "            df10 = df10.append({'polarity': \"neutral\"}, ignore_index=True)\n",
    "        if ll==0:\n",
    "            print(\"neither\")            \n",
    "df11=pd.concat([df4,df10],axis=1,join='inner',sort=False)  \n",
    "#print(df11)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " #change to numerical values\n",
    "df11.to_csv('annotated_csv', sep='\\t', encoding='utf-8') \n",
    "df11['polarity'] = df11.polarity.map(lambda x: int(2) if x =='positive' else int(0) if x =='negative' else int(1) if x == 'neutral' else np.nan)\n",
    "print(df11['polarity'].value_counts())\n",
    "print(df11)\n",
    "plt.hist(df11.polarity, bins = 3, align= 'mid')\n",
    "plt.xticks(range(3), ['Negative','Neutral', 'Positive'])\n",
    "plt.xlabel('Sentiment of Reviews')\n",
    "plt.title('Distribution of Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Random upsampling of the manority class(Two make two samples equal)\n",
    "df12=df11.sample(n=9138, replace=True, random_state=1)\n",
    "print(df12.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x=df12['tokenized_sents']\n",
    "y=df12['polarity']\n",
    "seed =7\n",
    "np.random.seed(seed)\n",
    "maximumlen=420 #maximum tweet length(no of words)\n",
    "# Truncate and pad the review sequences\n",
    "#Convert words to integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df12['tokenized_sents'].values)\n",
    "x = tokenizer.texts_to_sequences(df12['tokenized_sents'].values)\n",
    "vocab_size = len(tokenizer.word_index) + 1 \n",
    "print(\"vocab_size\",vocab_size)\n",
    "X = pad_sequences(x,maxlen=maximumlen)\n",
    "print(X.shape[0])\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.10,random_state=seed)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=seed)#validation data\n",
    "x_train=pad_sequences(x_train, maxlen=maximumlen)\n",
    "x_test=pad_sequences(x_test, maxlen=maximumlen)\n",
    "#padding x_val to maximum length\n",
    "x_val=pad_sequences(x_val, maxlen=maximumlen)\n",
    "print(x_train)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "model2 = gensim.models.Word2Vec(df11['tokenized_sents'],min_count = 1, size = 32,window = 5, sg = 1,iter=20,max_vocab_size=24200,seed=7) \n",
    "print(model2)\n",
    "#print(model2.vector_size)\n",
    "model2.save('w2v.model')\n",
    "#use the following lines of code to show similariy of words using word2vec\n",
    "#print(model2.similarity('ጥሩ','ምርጥ'))\n",
    "#print(model2.similarity('💚','💋'))\n",
    "#print(model2.wv.most_similar('💋'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topwords=15000 #top words to consider\n",
    "epoch=8         \n",
    "batch=14\n",
    "seed=7\n",
    "embedding_size = 8\n",
    "model=Sequential()\n",
    "np.random.seed(0)\n",
    "model.add(Embedding(topwords,8,input_length=maximumlen,dropout=0.5))\n",
    "model.add(LSTM(8,return_sequences=True,input_shape=(x_train.shape[0],x_train.shape[1])))\n",
    "model.add(Dropout(0.7))\n",
    "#model.add(LSTM(4,activation='softmax',return_sequences=True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(16, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "history=model.fit(x_train, y_train,validation_data=(x_val,y_val),epochs=epoch,batch_size=batch,verbose=2,\n",
    "                  callbacks=[EarlyStopping(monitor='val_loss', patience=5, baseline=0.3)])\n",
    "print(model.summary()) \n",
    "loss, accuracy = model.evaluate(x_test, y_test,verbose = 2, batch_size=batch) \n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "plt.style.use('ggplot')\n",
    "def plot_history(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('epoch number')\n",
    "    plt.ylabel('Training and validation loss')\n",
    "    plt.legend()\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the Evaluation Metrics values \n",
    "yhat_probs = model.predict(x_test, verbose=0)\n",
    "yhat_probs = np.array([np.argmax(pred) for pred in yhat_probs])\n",
    "# predict crisp classes for test set\n",
    "yhat_classes = model.predict_classes(x_test, verbose=0)\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test, yhat_classes,normalize=True)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_test, yhat_classes,labels=[0,1,2],average='macro') #average='micro' gives total score of Tp/tp+fp none gives each class precision\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test, yhat_classes,labels=[0,1,2],average='macro')\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test, yhat_classes,labels=[0,1,2],average='macro')\n",
    "print('F1 score: %f' % f1)\n",
    "# confusion matrix\n",
    "matrix = confusion_matrix(y_test,yhat_classes,labels=[0,1,2])\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the confusion matrics\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print(cm)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Predicted')\n",
    "# plot the confusion Matrix\n",
    "cm = confusion_matrix(y_test,yhat_classes)\n",
    "plot_confusion_matrix(cm, {'negative': 0, 'neutral':1,'positive':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
