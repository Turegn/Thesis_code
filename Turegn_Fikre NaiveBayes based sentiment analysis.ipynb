{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import copy,re,csv,time,nltk,emoji,string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report,precision_score,recall_score,f1_score,roc_auc_score,cohen_kappa_score,accuracy_score,confusion_matrix\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.initializers import Constant\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split,KFold, cross_val_score\n",
    "from keras.utils.np_utils import to_categorical \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer\n",
    "from nltk import bigrams, trigrams\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn import preprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  #Ignoring unnecessory warnings\n",
    "# importing our dataset to our project\n",
    "df=pd.read_csv('commentP_consN.csv',delimiter=';',names=['comment'],encoding=\"utf-8\")\n",
    "all=pd.DataFrame(df)\n",
    "print(all.info()) #information about our dataframe(comments)\n",
    "all= all.dropna(axis='columns', how='all') #remove empty rows from the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing of the dataset \n",
    "df['comment'] = df['comment'].str.strip()\n",
    "df['comment'] = df['comment'].str.replace('\\d+', '')\n",
    "df['comment'] = df['comment'].replace(regex={\"á‹\":\"áŠ \",\"á‹‘\":\"áŠ¡\",\"á‹’\":\"áŠ¢\",\"á‹“\":\"áŠ \",\"á‹”\":\"áŠ¤\",\"á‹•\":\"áŠ¥\",\"á‹–\":\"áŠ¦\",\n",
    "                                            \"á‹“\":\"áŠ \",\"áŠ½\":\"áˆ…\",\"áˆƒ\":\"áˆ€\",\"á‹‰\":\"á‹\",\n",
    "                                            \"áŒ¸\":\"á€\",\"áŒ¹\":\"á\",\"áŒº\":\"á‚\",\"áŒ»\":\"áƒ\",\"áŒ¼\":\"á„\",\"áŒ½\":\"á…\",\"áŒ¾\":\"á†\",\n",
    "                                             \"áŠƒ\":\"áˆ€\",\"áŠ\":\"áˆ\",\"áŠ‚\":\"áˆ‚\",\"áŠƒ\":\"áˆ€\",\"áŠ„\":\"áˆ„\",\"áŠ…\":\"áˆ…\",\"áŠ†\":\"áˆ†\",\n",
    "                                             \"áˆ \":\"áˆ°\",\"áˆ¡\":\"áˆ±\",\"áˆ¢\":\"áˆ²\",\"áˆ£\":\"áˆ³\",\"áˆ¤\":\"áˆ´\",\"áˆ¥\":\"áˆµ\",\"áˆ¦\":\"áˆ¶\",\n",
    "                                            \"áˆ\":\"áˆ€\",\"áˆ‘\":\"áˆ\",\"áˆ’\":\"áˆ‚\",\"áˆ“\":\"áˆ€\",\"áˆ”\":\"áˆ„\",\"áˆ•\":\"áˆ…\",\"áˆ–\":\"áˆ†\",                                             \n",
    "                                             \"áŠ®áˆ˜áŠ•á‰µ\":\"áŠ áˆµá‰°á‹«á‹¨á‰µ\",\"áˆ™á‰­\":\"áŠáˆáˆ\",\"áŠ¢á‰µá‹®áŒ½á‹«\":\"áŠ¢á‰µá‹®áŒµá‹«\",\"áŠ¢á‰µá‹¬\":\"áŠ¢á‰µá‹®áŒµá‹«\",\n",
    "                                             \"áŠ¢á‰²á‹®á’á‹«\":\"áŠ¢á‰µá‹®áŒµá‹«\",\"ááˆáˆ\":\"áŠáˆáˆ\",\"á‰ áŒ áˆ\":\"á‰ áŒ£áˆ\",\"1áŠ›\":\"áŠ áŠ•á‹°áŠ›\",\"á‹­áˆ˜á‰½áŠ­\":\"á‹­áˆ˜á‰½áˆ…\",\n",
    "                                             \"á‹­áˆáŠ•áˆ‹á‰¹\":\"á‹­áˆáŠ•áˆ‹á‰½áˆ\",\"áŠ áˆ¨\":\"áŠ¥áˆ¨\",\"áˆ‹á‹­áŠ­\":\"á‹‰á‹°á‹µ\",\"áá‰…áˆ­áˆ­áˆ­\":\"áá‰…áˆ­\",\"áá‰…áˆ­áˆ­\":\"áá‰…áˆ­\",\n",
    "                                             \"áˆ€áˆªá\":\"áŠ áˆªá\",\"áˆ€áˆªáˆ\":\"áŠ áˆªá\",\"áˆ°á‹á‹°á‹ˆ\":\"áˆµá‹ˆá‹°á‹\",\"á‹‹á‹á‹á‹á‹\":\"á‹‹á‹\",\"áŠ¡ááááááá\":\"áŠ¡áá\",\"áŠ¡áááááá\":\"áŠ¡áá\",\n",
    "                                             \"áŠ¡ááááá\":\"áŠ¡áá\",\"áŠ¡ááá\":\"áŠ¡á\",\"á‹á‹µá‹µá‹µá‹µá‹µ\":\"á‹á‹µá‹µ\",\n",
    "                                             \"á‹á‹µá‹µá‹µá‹µ\":\"á‹á‹µá‹µ\",\"á‹á‹µá‹µá‹µ\":\"á‹á‹µá‹µ\",\"á‹¶/áˆ­\":\"á‹¶áŠ­á‰°áˆ­\"})\n",
    "df['comment']=df['comment'].replace('{html}',\"\")\n",
    "freq = pd.Series(' '.join(df['comment']).split()).value_counts()[-10:]\n",
    "# comment the next line of codes when Emojis are not considered for comment labeling\n",
    "df['comment'] = [''.join(c for c in s if c not in string.punctuation) for s in df['comment']] \n",
    "# uncomment the next line of code when Emojis are not considered\n",
    "'''\n",
    "df['comment'] = df['comment'].str.replace('[^\\w\\s]','')\n",
    "for index, row in df.iterrows():\n",
    "     row=df['comment']==\"\"\n",
    "     index_no=df.index[row] #display index numbers(rows) that have element(values) \n",
    "aa=df['comment'].drop(index_no,axis=0)\n",
    "aa.to_csv('output.csv', index=False, header=None)\n",
    "dft=pd.read_csv('output.csv',delimiter=';',names=['comments'],encoding=\"utf-8\")\n",
    "afteremojiR=pd.DataFrame(dft) #changing to data frame\n",
    "print(afteremojiR)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft['tokenized_sents']= dft.apply(lambda row: nltk.word_tokenize(row['comments']), axis=1)\n",
    "dft['tokenized_sents'].replace('', np.nan, inplace=True)\n",
    "print(tabulate(dft[['tokenized_sents']]))\n",
    "frequency=dft['tokenized_sents']\n",
    "stop = stopwords.words('Amharic')\n",
    "df['stopwords'] = dft['tokenized_sents'].apply(lambda x: len([item for item in x if item in stop]))\n",
    "dft['tokenized_sents']=dft['tokenized_sents'].apply(lambda x: [item for item in x if item not in stop])\n",
    "dft['sents_length'] = dft.apply(lambda row: len(row['tokenized_sents']),axis=1)\n",
    "print(dft[['tokenized_sents','sents_length']])\n",
    "print(\"the maximum sents length is\",dft['sents_length'].max())\n",
    "df4=dft['tokenized_sents'] \n",
    "df5=pd.Series.to_frame(df4) #to change series to frame'\n",
    "df5.columns=['tokenized_sents']\n",
    "# import labeled positive,negative,intensifiers and invertor lexicons\n",
    "df2=pd.read_csv('Negative.csv',names=['negative'],encoding=\"utf8\")\n",
    "df7=pd.read_csv('positive.csv',names=['positive'],encoding=\"utf8\")\n",
    "df8=pd.read_csv('Invertor.csv',names=['Invertor'],encoding=\"utf8\")\n",
    "df9=pd.read_csv('intensifierA.csv',names=['intensifier'],encoding=\"utf8\")\n",
    "columen_names=['negative','positive','Invertor','intensifierA']\n",
    "result=pd.concat([df2,df7,df8,df9],axis=1,join='outer',sort=False,names=columen_names)\n",
    "blankIndex=[''] * len(result)\n",
    "result.index=blankIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft=pd.read_csv('output.csv',delimiter=';',names=['comments'],encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_count=0\n",
    "negative_count=0\n",
    "invertor_count=0\n",
    "intensifier_count=0\n",
    "pos_emoji=0\n",
    "positive_emoji_count=0\n",
    "negative_emoji_count=0\n",
    "emoji_score=0\n",
    "reviews_len = [len(x) for x in df['comment']]\n",
    "pd.Series(reviews_len).hist()\n",
    "print(plt.show())\n",
    "print(pd.Series(reviews_len).describe())\n",
    "df10 = pd.DataFrame(columns=['polarity'])\n",
    "# most appearing positive and negative labeled Emojis \n",
    "def extract_emoji(str):\n",
    "            return ''.join(c for c in str if c in emoji.UNICODE_EMOJI)\n",
    "positive_emoji=['ðŸ˜‚','â¤','â™¥','ðŸ˜','ðŸ˜˜','ðŸ˜Š','ðŸ‘Œ','ðŸ’•','ðŸ‘','ðŸ˜','â˜º','â™¡','ðŸ‘','ðŸ™','âœŒ','ðŸ˜','ðŸ˜‰','ðŸ™Œ','ðŸ™ˆ','âˆš', 'â¤ï¸','â™¥ï¸','â™¥ï¸',\n",
    "'ðŸ’ª','ðŸ˜„','ðŸ˜ƒ','ðŸ˜±','ðŸŽ‰','ðŸ˜œ','ðŸŒ¸','ðŸ’œ','ðŸ’™','ðŸ˜³','ðŸ’—','â˜€','ðŸ˜Ž','ðŸ˜¢','ðŸ’‹','ðŸ˜‹','ðŸ™Š','ðŸŽ¶','ðŸ’ž','ðŸ’',\n",
    "'ðŸ’¯','ðŸ’›','ðŸ’','ðŸ’š','ðŸ˜†','ðŸ˜','ðŸ˜…','ðŸ‘Š','ðŸ˜€','ðŸ˜š','ðŸ˜»','ðŸ’˜','ðŸ‘‹','âœ‹','ðŸŽŠ','â„','ðŸ˜¥','ðŸ˜ˆ',\n",
    "'ðŸ”','ðŸ˜°','âš½','ðŸ‘‘','ðŸ˜¹','ðŸƒ','ðŸŽ','ðŸ§','ðŸŽˆ','âœŠ','ðŸ’¤','ðŸ’“','ðŸ’¦','ðŸ™‹','ðŸŽ„','ðŸŽµ','ðŸ˜›','ðŸ˜¬','ðŸ‘¯',\n",
    "'ðŸ’Ž','ðŸŽ‚','ðŸ‘«','ðŸ†','â˜','ðŸ˜™','â›„','ðŸ‘…','â™ª','ðŸ‚','ðŸ’','ðŸŒ´','ðŸ‘ˆ','ðŸŒ¹','ðŸ™†','ðŸ’°','ðŸ»''ðŸŒž','ðŸ','â­',\n",
    "'ðŸŽ€','ðŸ™‰','ðŸ’…','ðŸŒº','ðŸ¶','ðŸŒš','ðŸŽ¤','ðŸ‘­','ðŸŽ§','ðŸ‘†','ðŸ¸','ðŸ˜‡','ðŸ‰','ðŸŽ¸','ðŸ¹','ðŸ’«','ðŸ“š','ðŸŒ·','ðŸ’',\n",
    "'ðŸ’¨','ðŸˆ','ðŸ’','â˜”','ðŸ‘¸','ðŸ‡ª','ðŸ˜¸','ðŸ”','ðŸ‘¼','ðŸ¯','ðŸ˜µ','ðŸ‘¶','ðŸ’','â†¾','ðŸ“–','ðŸ’','ðŸŒ','â”Š','ðŸ¥','ðŸ’„',\n",
    "'ðŸ’¸','â›”','ðŸ€','ðŸ’‰','ðŸ’Ÿ','ðŸ˜¯','â™¦','ðŸŒ™','ðŸŸ','ðŸ‘£','ðŸ—¿','ðŸ','ðŸ­','âŒ','ðŸ°','ðŸ’Š','ðŸš¨','ðŸª','âœ§','ðŸŽ†',\n",
    "'ðŸŽŽ','ðŸ‡©','âœ…','ðŸ”Š','ðŸ‘ ','ðŸŒŒ','ðŸŽ','ðŸ»','ðŸ’‡','ðŸŠ','ðŸ’','ðŸ­','ðŸ‘Ÿ','ðŸŒŽ','ðŸ','ðŸ®','ðŸ“²','ðŸŒ…','ðŸ‡·','ðŸŒ½',\n",
    "'ðŸ¬','ðŸ˜º','ðŸš€','Â¦ðŸ§','ðŸœ','ðŸ†—','ðŸ‹','âž¤','ðŸ„','ðŸ‘§','ðŸ','âœ','ðŸŒ¾','ðŸ¡','ðŸ‘™','â›…','ðŸ…','ðŸ“º','ðŸ','ðŸ‡®',\n",
    "'â™£ðŸ‡¹','ðŸ¬','ðŸŒ³','ðŸ’¿','ðŸ”','ðŸ¨','ðŸŒ•','ðŸ”µ','ðŸ³','ðŸš´','ðŸ‘°','âš“','ðŸ‘—','âž•','ðŸ’¬','ðŸ”œ','ðŸ¨','ðŸ™','ðŸ—','ðŸ²',\n",
    "'ðŸ˜¼','ðŸ™','ðŸ‘¨','ðŸš','ðŸ–','â™¨â–ƒ','ðŸš˜']\n",
    "negative_emoji=['ðŸ˜­',' ðŸ˜©','ðŸ˜’','ðŸ’–','ðŸ˜”','ðŸ˜¡','ðŸ˜´','ðŸ”«','ðŸ˜ž','ðŸ˜ª',' ðŸ˜«','ðŸ’€','ðŸ˜•',\n",
    "'ðŸ’”','ðŸ˜¤','ðŸ˜‘','ðŸ˜ ','ðŸ˜“','ðŸ˜£','ðŸ˜²','ðŸ˜¿','ðŸ˜' 'ðŸ˜¨','ðŸ˜·','ðŸ‘Ž','ðŸ’©','ðŸ™…','ðŸ˜¶','ðŸ”ª','ðŸ’ƒ',\n",
    "                'ðŸ‘¿','âœ‚','ðŸ‘ª','ðŸ˜¦','ðŸ£','ðŸ™','ðŸ’§','ðŸ˜¾','ðŸ¥','ðŸ˜­']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rules for comment labeling using both labeled Emoji and lexicon\n",
    "for (index_label, row_series) in df5.iterrows():\n",
    "    list=row_series.values  #accessing the row values\n",
    "    for i in range(len(list)):\n",
    "        ll=len(list[i])\n",
    "        li=list[i]\n",
    "        print(li)              \n",
    "        xx=extract_emoji(li)\n",
    "        y=[]\n",
    "        for s in xx:\n",
    "            y.append(s)\n",
    "            ''.join(y)\n",
    "        pos_emojiexist=[]\n",
    "        neg_emojiexist=[]\n",
    "        for element in positive_emoji:\n",
    "            for word in y:\n",
    "                pos_emojiexist.append(word in element)\n",
    "                positive_emoji_count=sum(pos_emojiexist)\n",
    "        for item in negative_emoji:\n",
    "            for word in y:\n",
    "                neg_emojiexist.append(word in item)\n",
    "                negative_emoji_count=sum(neg_emojiexist)\n",
    "        if len(y)!=0:\n",
    "            emoji_score=positive_emoji_count-negative_emoji_count\n",
    "        else:\n",
    "            emoji_score=0\n",
    "        positive_res=result.positive.isin(li)\n",
    "        negative_res=result.negative.isin(li)\n",
    "        invertor_res=result.Invertor.isin(li)\n",
    "        intensifierA_res=result.intensifier.isin(li)\n",
    "        positive_count=sum(positive_res)*2\n",
    "        negative_count=sum(negative_res)*2\n",
    "        invertor_count=sum(invertor_res)\n",
    "        intensifier_count=sum(intensifierA_res)\n",
    "        sentiment_score=positive_count-negative_count\n",
    "        if invertor_count>0 and intensifier_count==0 and sentiment_score>0 and  ll>0:\n",
    "            sentiment=-(sentiment_score)\n",
    "        if invertor_count>0 and intensifier_count==0 and sentiment_score<0  and ll>0:\n",
    "            sentiment=-(sentiment_score)\n",
    "        if intensifier_count>0 and invertor_count==0 and sentiment_score>0  and ll>0:\n",
    "            sentiment=sentiment_score+3\n",
    "        if intensifier_count> 0 and invertor_count==0 and sentiment_score <0 and ll>0:\n",
    "            sentiment=sentiment_score-3\n",
    "        if invertor_count==0 and intensifier_count==0 and sentiment_score >0 and ll>0:\n",
    "            sentiment=sentiment_score\n",
    "        if invertor_count==0 and intensifier_count==0 and sentiment_score <0 and   ll>0:\n",
    "            sentiment=sentiment_score\n",
    "        if intensifier_count> 0 and invertor_count> 0 and sentiment_score>0  and ll>0:\n",
    "            sentiment=-(sentiment_score)+3\n",
    "        if intensifier_count> 0 and invertor_count> 0 and sentiment_score<0 and ll>0:\n",
    "            sentiment=-(sentiment_score)-3\n",
    "        if intensifier_count> 0 and invertor_count==0 and sentiment_score==0 and ll>0:\n",
    "             sentiment=sentiment_score\n",
    "        if sentiment_score==0 and invertor_count==0 and intensifier_count==0 and ll>0:\n",
    "             sentiment=sentiment_score\n",
    "        if sentiment>0 and emoji_score>=0:\n",
    "            print(\"positive\")\n",
    "            df10 = df10.append({'polarity': \"positive\"}, ignore_index=True)\n",
    "        if sentiment==0 and emoji_score>0:\n",
    "            print(\"positive\")\n",
    "            df10 = df10.append({'polarity': \"positive\"}, ignore_index=True)\n",
    "        if sentiment<0 and emoji_score<=0:\n",
    "            print(\"negative\")\n",
    "            df10 = df10.append({'polarity': \"negative\"}, ignore_index=True)\n",
    "        if sentiment==0 and emoji_score<0:\n",
    "            print(\"negative\")\n",
    "            df10 = df10.append({'polarity': \"negative\"}, ignore_index=True)\n",
    "        if sentiment<0 and emoji_score>0:#no change word first\n",
    "            print(\"negative\")\n",
    "            df10 = df10.append({'polarity': \"negative\"}, ignore_index=True)\n",
    "        if sentiment > 0 and emoji_score<0: #word first\n",
    "            print(\"positive\")\n",
    "            df10 = df10.append({'polarity': \"positive\"}, ignore_index=True)\n",
    "        if sentiment==0 and emoji_score==0:\n",
    "            print(\"neutral\")\n",
    "            df10 = df10.append({'polarity': \"neutral\"}, ignore_index=True)\n",
    "        if ll==0:\n",
    "            print(\"neither\")            \n",
    "df11=pd.concat([df4,df10],axis=1,join='inner',sort=False)  \n",
    "print(df11)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11['polarity'] = df11.polarity.map(lambda x: int(2) if x =='positive' else int(0) if x =='negative' else int(1) if x == 'neutral' else np.nan)\n",
    "print(df11['polarity'].value_counts())\n",
    "print(df11)\n",
    "plt.hist(df11.polarity, bins = 3, align= 'mid')\n",
    "plt.xticks(range(3), ['Negative','Neutral', 'Positive'])\n",
    "plt.xlabel('Sentiment of Reviews')\n",
    "plt.title('Distribution of Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"sentiment.csv\",\"w+\")\n",
    "fulll=[]\n",
    "for i in range(len(df11.index)):\n",
    "    x=df11.iat[i,0]\n",
    "    full = ' '.join(x)\n",
    "    f.write(full)\n",
    "    f.write('\\n')\n",
    "f.close()\n",
    "f=open(\"sentiment.csv\",\"r\")\n",
    "df=pd.read_csv('sentiment.csv',delimiter=';',names=['text'],encoding=\"utf-8\")\n",
    "all=pd.DataFrame(df)\n",
    "vectorizer=CountVectorizer()\n",
    "#Finding the bigram representation \n",
    "bigram_vectorizer=CountVectorizer(ngram_range=(1,2),max_features=24200)\n",
    "X=bigram_vectorizer.fit_transform(df['text']).toarray()\n",
    "print(X.shape)\n",
    "y=df11['polarity'].astype('int')\n",
    "print(y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 7)\n",
    "model =MultinomialNB() # use GaussianNB,BernoulliNB for other Naive Bayes models\n",
    "model.fit(X_train, y_train)\n",
    "expected = y_test\n",
    "predicted = model.predict(X_test)\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(expected, predicted))\n",
    "n=X_test.shape[0]\n",
    "error=np.sum(y_test!=predicted)\n",
    "error=error*100\n",
    "error=error/float(n)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the confusion matrix\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print(cm)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout() \n",
    "    plt.ylabel('True')  \n",
    "    plt.xlabel('Predicted')\n",
    "# plot the confusion Matrix\n",
    "cm = confusion_matrix(expected,predicted)\n",
    "plot_confusion_matrix(cm, {'negative': 0, 'neutral':1,'positive':2})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
