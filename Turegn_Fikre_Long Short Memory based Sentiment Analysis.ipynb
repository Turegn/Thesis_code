{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages for the project\n",
    "import copy,csv,time,nltk,emoji,gensim,string \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from tabulate import tabulate  \n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords  \n",
    "import matplotlib.pyplot as plt \n",
    "from gensim.models import word2vec \n",
    "import gensim.models.keyedvectors as word2vec #need to use due to depreceated model\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras import models,layers  \n",
    "from keras.layers import Dense,Dropout,LSTM,Activation,Bidirectional,Flatten,Embedding\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer,one_hot,text_to_word_sequence\n",
    "from keras.initializers import Constant \n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import classification_report,precision_score,recall_score,f1_score,roc_auc_score,cohen_kappa_score,accuracy_score,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split,KFold,cross_val_score \n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.datasets import make_circles\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow import set_random_seed\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer\n",
    "from nltk import bigrams, trigrams\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import warnings\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "warnings.filterwarnings(\"ignore\")  #Ignoring unnecessory warnings\n",
    "# import the dataset to our project \n",
    "df=pd.read_csv('commentP_consN.csv',delimiter=';',names=['comment'],encoding=\"utf8\")\n",
    "all=pd.DataFrame(df) #changing to data frame \n",
    "print(all) #display the first n rows of comments\n",
    "print(all.info()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describition before preprocessing \n",
    "reviews_len = [len(x) for x in df['comment']]\n",
    "pd.Series(reviews_len).hist()\n",
    "print(plt.show())\n",
    "print(pd.Series(reviews_len).describe())\n",
    "# remove the comment to identify sum of words and maximum word before preprocessing \n",
    "'''\n",
    "df['tokenized_sents']= df.apply(lambda row: nltk.word_tokenize(row['comment']), axis=1)\n",
    "df['sents_length'] = df.apply(lambda row: len(row['tokenized_sents']),axis=1)\n",
    "print(df[['tokenized_sents','sents_length']])\n",
    "print(\"sum of words is\",sum(df['sents_length']))\n",
    "print(\"the maximum sents and minimum length is\",df['sents_length'].max(),df['sents_length'].min())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove digits \n",
    "df['comment'] = df['comment'].str.replace('\\d+', '')\n",
    "#print(df[['comment']])\n",
    "#the following is normalization of words and characters\n",
    "df['comment'] = df['comment'].replace(regex={\"á‹\":\"áŠ \",\"á‹‘\":\"áŠ¡\",\"á‹’\":\"áŠ¢\",\"á‹“\":\"áŠ \",\"á‹”\":\"áŠ¤\",\"á‹•\":\"áŠ¥\",\"á‹–\":\"áŠ¦\",\n",
    "                                            \"á‹“\":\"áŠ \",\"áŠ½\":\"áˆ…\",\"áˆƒ\":\"áˆ€\",\"á‹‰\":\"á‹\",\n",
    "                                            \"áŒ¸\":\"á€\",\"áŒ¹\":\"á\",\"áŒº\":\"á‚\",\"áŒ»\":\"áƒ\",\"áŒ¼\":\"á„\",\"áŒ½\":\"á…\",\"áŒ¾\":\"á†\",\n",
    "                                             \"áŠƒ\":\"áˆ€\",\"áŠ\":\"áˆ\",\"áŠ‚\":\"áˆ‚\",\"áŠƒ\":\"áˆ€\",\"áŠ„\":\"áˆ„\",\"áŠ…\":\"áˆ…\",\"áŠ†\":\"áˆ†\",\n",
    "                                             \"áˆ \":\"áˆ°\",\"áˆ¡\":\"áˆ±\",\"áˆ¢\":\"áˆ²\",\"áˆ£\":\"áˆ³\",\"áˆ¤\":\"áˆ´\",\"áˆ¥\":\"áˆµ\",\"áˆ¦\":\"áˆ¶\",\n",
    "                                            \"áˆ\":\"áˆ€\",\"áˆ‘\":\"áˆ\",\"áˆ’\":\"áˆ‚\",\"áˆ“\":\"áˆ€\",\"áˆ”\":\"áˆ„\",\"áˆ•\":\"áˆ…\",\"áˆ–\":\"áˆ†\",                                             \n",
    "                                             \"áŠ®áˆ˜áŠ•á‰µ\":\"áŠ áˆµá‰°á‹«á‹¨á‰µ\",\"áˆ™á‰­\":\"áŠáˆáˆ\",\"áŠ¢á‰µá‹®áŒ½á‹«\":\"áŠ¢á‰µá‹®áŒµá‹«\",\"áŠ¢á‰µá‹¬\":\"áŠ¢á‰µá‹®áŒµá‹«\",\n",
    "                                             \"áŠ¢á‰²á‹®á’á‹«\":\"áŠ¢á‰µá‹®áŒµá‹«\",\"ááˆáˆ\":\"áŠáˆáˆ\",\"á‰ áŒ áˆ\":\"á‰ áŒ£áˆ\",\"1áŠ›\":\"áŠ áŠ•á‹°áŠ›\",\"á‹­áˆ˜á‰½áŠ­\":\"á‹­áˆ˜á‰½áˆ…\",\n",
    "                                             \"á‹­áˆáŠ•áˆ‹á‰¹\":\"á‹­áˆáŠ•áˆ‹á‰½áˆ\",\"áŠ áˆ¨\":\"áŠ¥áˆ¨\",\"áˆ‹á‹­áŠ­\":\"á‹‰á‹°á‹µ\",\"áá‰…áˆ­áˆ­áˆ­\":\"áá‰…áˆ­\",\"áá‰…áˆ­áˆ­\":\"áá‰…áˆ­\",\n",
    "                                             \"áˆ€áˆªá\":\"áŠ áˆªá\",\"áˆ€áˆªáˆ\":\"áŠ áˆªá\",\"áˆ°á‹á‹°á‹ˆ\":\"áˆµá‹ˆá‹°á‹\",\"á‹‹á‹á‹á‹á‹\":\"á‹‹á‹\",\"áŠ¡ááááááá\":\"áŠ¡áá\",\"áŠ¡áááááá\":\"áŠ¡áá\",\n",
    "                                             \"áŠ¡ááááá\":\"áŠ¡áá\",\"áŠ¡ááá\":\"áŠ¡á\",\"á‹á‹µá‹µá‹µá‹µá‹µ\":\"á‹á‹µá‹µ\",\n",
    "                                            \"á‹á‹µá‹µá‹µá‹µ\":\"á‹á‹µá‹µ\",\"á‹á‹µá‹µá‹µ\":\"á‹á‹µá‹µ\",\"1áŠ›\":\"áŠ áŠ•á‹°áŠ›\",\"áŠ¢á‰µá‹©á‰¢á‹«á‹Š\":\"áŠ¢á‰µá‹®áŒ½á‹«á‹Š\"})\n",
    "print(df[['comment']])\n",
    "#removing htmls from each comment \n",
    "df['comment']=df['comment'].replace('{html}',\"\") \n",
    "freq = pd.Series(' '.join(df['comment']).split()).value_counts()[-10:]\n",
    "# uncomment the next line of code when Emojis are not considered\n",
    "df['comment'] = [''.join(c for c in s if c not in string.punctuation) for s in df['comment']]\n",
    "# uncomment the following line of codes when considering Emojis for comment labeling \n",
    "''' \n",
    "#df['comment'] = df['comment'].str.replace('[^\\w\\s]','') # for removing Emojis from the dataset uncomment it.\n",
    "for index, row in df.iterrows():\n",
    "     row=df['comment']==\"\"\n",
    "     index_no=df.index[row] #display index numbers(rows) that have element(values) \n",
    "aa=df['comment'].drop(index_no,axis=0)\n",
    "aa.to_csv('output.csv', index=False, header=None)\n",
    "df=pd.read_csv('output.csv',delimiter=';',names=['comment'],encoding=\"utf-8\")\n",
    "afteremojiR=pd.DataFrame(df) #changing to data frame\n",
    "print(afteremojiR)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing of the comments \n",
    "df['tokenized_sents']= df.apply(lambda row: nltk.word_tokenize(row['comment']), axis=1)\n",
    "# removing na values(containing empty values)\n",
    "df['tokenized_sents'].replace('', np.nan, inplace=True)\n",
    "#print(tabulate(dft['tokenized_sents']))\n",
    "frequency=df['tokenized_sents']\n",
    "# Removing Amharic Stopwords\n",
    "stop = stopwords.words('Amharic')\n",
    "df['stopwords'] = df['tokenized_sents'].apply(lambda x: len([item for item in x if item in stop]))\n",
    "df['tokenized_sents']=df['tokenized_sents'].apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sents_length'] = df.apply(lambda row: len(row['tokenized_sents']),axis=1)\n",
    "print(df[['tokenized_sents','sents_length']])\n",
    "print(\"sum of words is\",sum(df['sents_length']))\n",
    "print(\"the maximum sents length is\",df['sents_length'].sum())\n",
    "df4=df['tokenized_sents']\n",
    "df5=pd.Series.to_frame(df4) #to change series to frame'\n",
    "df5.columns=['tokenized_sents']\n",
    "#dfp=pd.read_csv('positive_emoji.csv',delimiter=';',names=['positive_emoji'],encoding=\"utf8\")\n",
    "#dfn=pd.read_csv('negative_emoji.csv',delimiter=';',names=['negative_emoji'],encoding=\"utf8\")\n",
    "df2=pd.read_csv('Negative.csv',names=['negative'],encoding=\"utf8\")\n",
    "df7=pd.read_csv('positive.csv',names=['positive'],encoding=\"utf8\")\n",
    "df8=pd.read_csv('Invertor.csv',names=['Invertor'],encoding=\"utf8\")\n",
    "df9=pd.read_csv('intensifierA.csv',names=['intensifier'],encoding=\"utf8\")\n",
    "columen_names=['negative','positive','Invertor','intensifierA']\n",
    "result=pd.concat([df2,df7,df8,df9],axis=1,join='outer',sort=False,names=columen_names)\n",
    "blankIndex=[''] * len(result)\n",
    "result.index=blankIndex\n",
    "#information about lexicons \n",
    "print(\"number of positive lexicon\",df8.info())\n",
    "print(\"number of negative lexicon\",df9.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_count=0\n",
    "negative_count=0\n",
    "invertor_count=0\n",
    "intensifier_count=0\n",
    "pos_emoji=0\n",
    "positive_emoji_count=0\n",
    "negative_emoji_count=0\n",
    "emoji_score=0\n",
    "#review length analysis used to include or exclude very short or lengthy statements\n",
    "reviews_len = [len(x) for x in df['comment']]\n",
    "pd.Series(reviews_len).hist()\n",
    "print(plt.show())\n",
    "print(pd.Series(reviews_len).describe())\n",
    "df10 = pd.DataFrame(columns=['polarity'])\n",
    "# The following are list of labeled positive and negative Emojis\n",
    "def extract_emoji(str):\n",
    "            return ''.join(c for c in str if c in emoji.UNICODE_EMOJI)\n",
    "positive_emoji=['ðŸ˜‚','â¤','â™¥','ðŸ˜','ðŸ˜˜','ðŸ˜Š','ðŸ‘Œ','ðŸ’•','ðŸ‘','ðŸ˜','â˜º','â™¡','ðŸ‘','ðŸ™','âœŒ','ðŸ˜',\n",
    "                'ðŸ˜‰','ðŸ™Œ','ðŸ™ˆ','âˆš', 'â¤ï¸','â™¥ï¸','â™¥ï¸','â¤ï¸',\n",
    "'ðŸ’ª','ðŸ˜„','ðŸ˜ƒ','ðŸ˜±','ðŸŽ‰','ðŸ˜œ','ðŸŒ¸','ðŸ’œ','ðŸ’™','ðŸ˜³','ðŸ’—','â˜€','ðŸ˜Ž','ðŸ˜¢','ðŸ’‹','ðŸ˜‹','ðŸ™Š','ðŸŽ¶','ðŸ’ž','ðŸ’'\n",
    "'ðŸ˜Œ','ðŸ’¯','ðŸ’›','ðŸ’','ðŸ’š','ðŸ˜†','ðŸ˜','ðŸ˜…','ðŸ‘Š','ðŸ˜€','ðŸ˜š','ðŸ˜»','ðŸ’˜','ðŸ‘‹','âœ‹','ðŸŽŠ','â„','ðŸ˜¥','ðŸ˜ˆ',\n",
    "'ðŸ”','ðŸ˜°','âš½','ðŸ‘‘','ðŸ˜¹','ðŸƒ','ðŸŽ','ðŸ§','ðŸŽˆ','âœŠ','ðŸ’¤','ðŸ’“','ðŸ’¦','ðŸ™‹','ðŸŽ„','ðŸŽµ','ðŸ˜›','ðŸ˜¬','ðŸ‘¯',\n",
    "'ðŸ’Ž','ðŸŽ‚','ðŸ‘«','ðŸ†','â˜','ðŸ˜™','â›„','ðŸ‘…','â™ª','ðŸ‚','ðŸ’','ðŸŒ´','ðŸ‘ˆ','ðŸŒ¹','ðŸ™†','ðŸ’°','ðŸ»''ðŸŒž','ðŸ','â­',\n",
    "'ðŸŽ€','ðŸ™‰','ðŸ’…','ðŸŒº','ðŸ¶','ðŸŒš','ðŸŽ¤','ðŸ‘­','ðŸŽ§','ðŸ‘†','ðŸ¸','ðŸ˜‡','ðŸ‰','ðŸŽ¸','ðŸ¹','ðŸ’«','ðŸ“š','ðŸŒ·','ðŸ’',\n",
    "'ðŸ’¨','ðŸˆ','ðŸ’','â˜”','ðŸ‘¸','ðŸ‡ª','ðŸ˜¸','ðŸ”','ðŸ‘¼','ðŸ¯','ðŸ˜µ','ðŸ‘¶','ðŸ’','â†¾','ðŸ“–','ðŸ’','ðŸŒ','â”Š','ðŸ¥','ðŸ’„',\n",
    "'ðŸ’¸','â›”','ðŸ€','ðŸ’‰','ðŸ’Ÿ','ðŸ˜¯','â™¦','ðŸŒ™','ðŸŸ','ðŸ‘£','ðŸ—¿','ðŸ','ðŸ­','âŒ','ðŸ°','ðŸ’Š','ðŸš¨','ðŸª','âœ§','ðŸŽ†',\n",
    "'ðŸŽŽ','ðŸ‡©','âœ…','ðŸ”Š','ðŸ‘ ','ðŸŒŒ','ðŸŽ','ðŸ»','ðŸ’‡','ðŸŠ','ðŸ’','ðŸ­','ðŸ‘Ÿ','ðŸŒŽ','ðŸ','ðŸ®','ðŸ“²','ðŸŒ…','ðŸ‡·','ðŸŒ½',\n",
    "'ðŸ¬','ðŸ˜º','ðŸš€','Â¦ðŸ§','ðŸœ','ðŸ†—','ðŸ‹','âž¤','ðŸ„','ðŸ‘§','ðŸ','âœ','ðŸŒ¾','ðŸ¡','ðŸ‘™','â›…','ðŸ…','ðŸ“º','ðŸ','ðŸ‡®',\n",
    "'â™£ðŸ‡¹','ðŸ¬','ðŸŒ³','ðŸ’¿','ðŸ”','ðŸ¨','ðŸŒ•','ðŸ”µ','ðŸ³','ðŸš´','ðŸ‘°','âš“','ðŸ‘—','âž•','ðŸ’¬','ðŸ”œ','ðŸ¨','ðŸ™','ðŸ—','ðŸ²',\n",
    "'ðŸ˜¼','ðŸ™','ðŸ‘¨','ðŸš','ðŸ–','â™¨â–ƒ','ðŸš˜']\n",
    "negative_emoji=['ðŸ˜­',' ðŸ˜©','ðŸ˜’','ðŸ’–','ðŸ˜”','ðŸ˜¡','ðŸ˜´','ðŸ”«','ðŸ˜ž','ðŸ˜ª',' ðŸ˜«','ðŸ’€','ðŸ˜•',\n",
    "'ðŸ’”','ðŸ˜¤','ðŸ˜‘','ðŸ˜ ','ðŸ˜“','ðŸ˜£','ðŸ˜²','ðŸ˜¿','ðŸ˜' 'ðŸ˜¨','ðŸ˜·','ðŸ‘Ž','ðŸ’©','ðŸ™…','ðŸ˜¶','ðŸ”ª','ðŸ’ƒ',\n",
    "                'ðŸ‘¿','âœ‚','ðŸ‘ª','ðŸ˜¦','ðŸ£','ðŸ™','ðŸ’§','ðŸ˜¾','ðŸ¥','ðŸ˜­'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rules for labeling comments using labeled lexicons and Emoji\n",
    "for (index_label, row_series) in df5.iterrows():\n",
    "    list=row_series.values  #accessing the row values\n",
    "    for i in range(len(list)):\n",
    "         #print(list[i])\n",
    "        ll=len(list[i])\n",
    "        #print(ll)\n",
    "        li=list[i]\n",
    "        print(li)              #if df2['negative'].isin(li[j]) \n",
    "        xx=extract_emoji(li)\n",
    "        #print(xx)    \n",
    "        y=[]\n",
    "        for s in xx:\n",
    "            y.append(s)\n",
    "            ''.join(y)\n",
    "        #print(y)\n",
    "        pos_emojiexist=[]\n",
    "        neg_emojiexist=[]\n",
    "        for element in positive_emoji:\n",
    "            for word in y:\n",
    "                pos_emojiexist.append(word in element)\n",
    "                positive_emoji_count=sum(pos_emojiexist)\n",
    "        for item in negative_emoji:\n",
    "            for word in y:\n",
    "                neg_emojiexist.append(word in item)\n",
    "                negative_emoji_count=sum(neg_emojiexist)\n",
    "        if len(y)!=0:\n",
    "            emoji_score=positive_emoji_count-negative_emoji_count\n",
    "        else:\n",
    "            emoji_score=0\n",
    "        positive_res=result.positive.isin(li)\n",
    "        negative_res=result.negative.isin(li)\n",
    "        invertor_res=result.Invertor.isin(li)\n",
    "        intensifierA_res=result.intensifier.isin(li)\n",
    "        positive_count=sum(positive_res)*2\n",
    "        negative_count=sum(negative_res)*2\n",
    "        invertor_count=sum(invertor_res)\n",
    "        intensifier_count=sum(intensifierA_res)\n",
    "        sentiment_score=positive_count-negative_count\n",
    "        if invertor_count>0 and intensifier_count==0 and sentiment_score>0 and  ll>0:\n",
    "            sentiment=-(sentiment_score)\n",
    "        if invertor_count>0 and intensifier_count==0 and sentiment_score<0  and ll>0:\n",
    "            sentiment=-(sentiment_score)\n",
    "        if intensifier_count>0 and invertor_count==0 and sentiment_score>0  and ll>0:\n",
    "            sentiment=sentiment_score+3\n",
    "        if intensifier_count> 0 and invertor_count==0 and sentiment_score <0 and ll>0:\n",
    "            sentiment=sentiment_score-3\n",
    "        if invertor_count==0 and intensifier_count==0 and sentiment_score >0 and ll>0:\n",
    "            sentiment=sentiment_score\n",
    "        if invertor_count==0 and intensifier_count==0 and sentiment_score <0 and   ll>0:\n",
    "            sentiment=sentiment_score\n",
    "        if intensifier_count> 0 and invertor_count> 0 and sentiment_score>0  and ll>0:\n",
    "            sentiment=-(sentiment_score)+3\n",
    "        if intensifier_count> 0 and invertor_count> 0 and sentiment_score<0 and ll>0:\n",
    "            sentiment=-(sentiment_score)-3\n",
    "        if intensifier_count> 0 and invertor_count==0 and sentiment_score==0 and ll>0:\n",
    "             sentiment=sentiment_score\n",
    "        if sentiment_score==0 and invertor_count==0 and intensifier_count==0 and ll>0:\n",
    "             sentiment=sentiment_score\n",
    "        if sentiment>0 and emoji_score>=0:\n",
    "            print(\"positive\")\n",
    "            df10 = df10.append({'polarity': \"positive\"}, ignore_index=True)\n",
    "        if sentiment==0 and emoji_score>0:\n",
    "            print(\"positive\")\n",
    "            df10 = df10.append({'polarity': \"positive\"}, ignore_index=True)\n",
    "        if sentiment<0 and emoji_score<=0:\n",
    "            print(\"negative\")\n",
    "            df10 = df10.append({'polarity': \"negative\"}, ignore_index=True)\n",
    "        if sentiment==0 and emoji_score<0:\n",
    "            print(\"negative\")\n",
    "            df10 = df10.append({'polarity': \"negative\"}, ignore_index=True)\n",
    "        if sentiment<0 and emoji_score>0:#no change word first\n",
    "            print(\"negative\")\n",
    "            df10 = df10.append({'polarity': \"negative\"}, ignore_index=True)\n",
    "        if sentiment > 0 and emoji_score<0: #word first\n",
    "            print(\"positive\")\n",
    "            df10 = df10.append({'polarity': \"positive\"}, ignore_index=True)\n",
    "        if sentiment==0 and emoji_score==0:\n",
    "            print(\"neutral\")\n",
    "            df10 = df10.append({'polarity': \"neutral\"}, ignore_index=True)\n",
    "        if ll==0:\n",
    "            print(\"neither\")            \n",
    "df11=pd.concat([df4,df10],axis=1,join='inner',sort=False)  \n",
    "#print(df11)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " #change to numerical values\n",
    "df11.to_csv('annotated_csv', sep='\\t', encoding='utf-8') \n",
    "df11['polarity'] = df11.polarity.map(lambda x: int(2) if x =='positive' else int(0) if x =='negative' else int(1) if x == 'neutral' else np.nan)\n",
    "print(df11['polarity'].value_counts())\n",
    "print(df11)\n",
    "plt.hist(df11.polarity, bins = 3, align= 'mid')\n",
    "plt.xticks(range(3), ['Negative','Neutral', 'Positive'])\n",
    "plt.xlabel('Sentiment of Reviews')\n",
    "plt.title('Distribution of Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Random upsampling of the manority class(Two make two samples equal)\n",
    "df12=df11.sample(n=9138, replace=True, random_state=1)\n",
    "print(df12.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x=df12['tokenized_sents']\n",
    "y=df12['polarity']\n",
    "seed =7\n",
    "np.random.seed(seed)\n",
    "maximumlen=420 #maximum tweet length(no of words)\n",
    "# Truncate and pad the review sequences\n",
    "#Convert words to integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df12['tokenized_sents'].values)\n",
    "x = tokenizer.texts_to_sequences(df12['tokenized_sents'].values)\n",
    "vocab_size = len(tokenizer.word_index) + 1 \n",
    "print(\"vocab_size\",vocab_size)\n",
    "X = pad_sequences(x,maxlen=maximumlen)\n",
    "print(X.shape[0])\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.10,random_state=seed)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=seed)#validation data\n",
    "x_train=pad_sequences(x_train, maxlen=maximumlen)\n",
    "x_test=pad_sequences(x_test, maxlen=maximumlen)\n",
    "#padding x_val to maximum length\n",
    "x_val=pad_sequences(x_val, maxlen=maximumlen)\n",
    "print(x_train)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "model2 = gensim.models.Word2Vec(df11['tokenized_sents'],min_count = 1, size = 32,window = 5, sg = 1,iter=20,max_vocab_size=24200,seed=7) \n",
    "print(model2)\n",
    "#print(model2.vector_size)\n",
    "model2.save('w2v.model')\n",
    "#use the following lines of code to show similariy of words using word2vec\n",
    "#print(model2.similarity('áŒ¥áˆ©','áˆáˆ­áŒ¥'))\n",
    "#print(model2.similarity('ðŸ’š','ðŸ’‹'))\n",
    "#print(model2.wv.most_similar('ðŸ’‹'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topwords=15000 #top words to consider\n",
    "epoch=8         \n",
    "batch=14\n",
    "seed=7\n",
    "embedding_size = 8\n",
    "model=Sequential()\n",
    "np.random.seed(0)\n",
    "model.add(Embedding(topwords,8,input_length=maximumlen,dropout=0.5))\n",
    "model.add(LSTM(8,return_sequences=True,input_shape=(x_train.shape[0],x_train.shape[1])))\n",
    "model.add(Dropout(0.7))\n",
    "#model.add(LSTM(4,activation='softmax',return_sequences=True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(16, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "history=model.fit(x_train, y_train,validation_data=(x_val,y_val),epochs=epoch,batch_size=batch,verbose=2,\n",
    "                  callbacks=[EarlyStopping(monitor='val_loss', patience=5, baseline=0.3)])\n",
    "print(model.summary()) \n",
    "loss, accuracy = model.evaluate(x_test, y_test,verbose = 2, batch_size=batch) \n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "plt.style.use('ggplot')\n",
    "def plot_history(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('epoch number')\n",
    "    plt.ylabel('Training and validation loss')\n",
    "    plt.legend()\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the Evaluation Metrics values \n",
    "yhat_probs = model.predict(x_test, verbose=0)\n",
    "yhat_probs = np.array([np.argmax(pred) for pred in yhat_probs])\n",
    "# predict crisp classes for test set\n",
    "yhat_classes = model.predict_classes(x_test, verbose=0)\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test, yhat_classes,normalize=True)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_test, yhat_classes,labels=[0,1,2],average='macro') #average='micro' gives total score of Tp/tp+fp none gives each class precision\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test, yhat_classes,labels=[0,1,2],average='macro')\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test, yhat_classes,labels=[0,1,2],average='macro')\n",
    "print('F1 score: %f' % f1)\n",
    "# confusion matrix\n",
    "matrix = confusion_matrix(y_test,yhat_classes,labels=[0,1,2])\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the confusion matrics\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print(cm)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Predicted')\n",
    "# plot the confusion Matrix\n",
    "cm = confusion_matrix(y_test,yhat_classes)\n",
    "plot_confusion_matrix(cm, {'negative': 0, 'neutral':1,'positive':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
