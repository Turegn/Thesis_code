{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import copy,re,csv,time,nltk,emoji,string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report,precision_score,recall_score,f1_score,roc_auc_score,cohen_kappa_score,accuracy_score,confusion_matrix\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.initializers import Constant\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split,KFold, cross_val_score\n",
    "from keras.utils.np_utils import to_categorical \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer\n",
    "from nltk import bigrams, trigrams\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn import preprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  #Ignoring unnecessory warnings\n",
    "# importing our dataset to our project\n",
    "df=pd.read_csv('commentP_consN.csv',delimiter=';',names=['comment'],encoding=\"utf-8\")\n",
    "all=pd.DataFrame(df)\n",
    "print(all.info()) #information about our dataframe(comments)\n",
    "all= all.dropna(axis='columns', how='all') #remove empty rows from the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing of the dataset \n",
    "df['comment'] = df['comment'].str.strip()\n",
    "df['comment'] = df['comment'].str.replace('\\d+', '')\n",
    "df['comment'] = df['comment'].replace(regex={\"ዐ\":\"አ\",\"ዑ\":\"ኡ\",\"ዒ\":\"ኢ\",\"ዓ\":\"አ\",\"ዔ\":\"ኤ\",\"ዕ\":\"እ\",\"ዖ\":\"ኦ\",\n",
    "                                            \"ዓ\":\"አ\",\"ኽ\":\"ህ\",\"ሃ\":\"ሀ\",\"ዉ\":\"ው\",\n",
    "                                            \"ጸ\":\"ፀ\",\"ጹ\":\"ፁ\",\"ጺ\":\"ፂ\",\"ጻ\":\"ፃ\",\"ጼ\":\"ፄ\",\"ጽ\":\"ፅ\",\"ጾ\":\"ፆ\",\n",
    "                                             \"ኃ\":\"ሀ\",\"ኁ\":\"ሁ\",\"ኂ\":\"ሂ\",\"ኃ\":\"ሀ\",\"ኄ\":\"ሄ\",\"ኅ\":\"ህ\",\"ኆ\":\"ሆ\",\n",
    "                                             \"ሠ\":\"ሰ\",\"ሡ\":\"ሱ\",\"ሢ\":\"ሲ\",\"ሣ\":\"ሳ\",\"ሤ\":\"ሴ\",\"ሥ\":\"ስ\",\"ሦ\":\"ሶ\",\n",
    "                                            \"ሐ\":\"ሀ\",\"ሑ\":\"ሁ\",\"ሒ\":\"ሂ\",\"ሓ\":\"ሀ\",\"ሔ\":\"ሄ\",\"ሕ\":\"ህ\",\"ሖ\":\"ሆ\",                                             \n",
    "                                             \"ኮመንት\":\"አስተያየት\",\"ሙቭ\":\"ፊልም\",\"ኢትዮጽያ\":\"ኢትዮጵያ\",\"ኢትዬ\":\"ኢትዮጵያ\",\n",
    "                                             \"ኢቲዮፒያ\":\"ኢትዮጵያ\",\"ፍልም\":\"ፊልም\",\"በጠም\":\"በጣም\",\"1ኛ\":\"አንደኛ\",\"ይመችክ\":\"ይመችህ\",\n",
    "                                             \"ይሁንላቹ\":\"ይሁንላችሁ\",\"አረ\":\"እረ\",\"ላይክ\":\"ዉደድ\",\"ፍቅርርር\":\"ፍቅር\",\"ፍቅርር\":\"ፍቅር\",\n",
    "                                             \"ሀሪፍ\":\"አሪፍ\",\"ሀሪፈ\":\"አሪፍ\",\"ሰውደወ\":\"ስወደው\",\"ዋውውውው\":\"ዋው\",\"ኡፍፍፍፍፍፍፍ\":\"ኡፍፍ\",\"ኡፍፍፍፍፍፍ\":\"ኡፍፍ\",\n",
    "                                             \"ኡፍፍፍፍፍ\":\"ኡፍፍ\",\"ኡፍፍፍ\":\"ኡፍ\",\"ውድድድድድ\":\"ውድድ\",\n",
    "                                             \"ውድድድድ\":\"ውድድ\",\"ውድድድ\":\"ውድድ\",\"ዶ/ር\":\"ዶክተር\"})\n",
    "df['comment']=df['comment'].replace('{html}',\"\")\n",
    "freq = pd.Series(' '.join(df['comment']).split()).value_counts()[-10:]\n",
    "# comment the next line of codes when Emojis are not considered for comment labeling\n",
    "df['comment'] = [''.join(c for c in s if c not in string.punctuation) for s in df['comment']] \n",
    "# uncomment the next line of code when Emojis are not considered\n",
    "'''\n",
    "df['comment'] = df['comment'].str.replace('[^\\w\\s]','')\n",
    "for index, row in df.iterrows():\n",
    "     row=df['comment']==\"\"\n",
    "     index_no=df.index[row] #display index numbers(rows) that have element(values) \n",
    "aa=df['comment'].drop(index_no,axis=0)\n",
    "aa.to_csv('output.csv', index=False, header=None)\n",
    "dft=pd.read_csv('output.csv',delimiter=';',names=['comments'],encoding=\"utf-8\")\n",
    "afteremojiR=pd.DataFrame(dft) #changing to data frame\n",
    "print(afteremojiR)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft['tokenized_sents']= dft.apply(lambda row: nltk.word_tokenize(row['comments']), axis=1)\n",
    "dft['tokenized_sents'].replace('', np.nan, inplace=True)\n",
    "print(tabulate(dft[['tokenized_sents']]))\n",
    "frequency=dft['tokenized_sents']\n",
    "stop = stopwords.words('Amharic')\n",
    "df['stopwords'] = dft['tokenized_sents'].apply(lambda x: len([item for item in x if item in stop]))\n",
    "dft['tokenized_sents']=dft['tokenized_sents'].apply(lambda x: [item for item in x if item not in stop])\n",
    "dft['sents_length'] = dft.apply(lambda row: len(row['tokenized_sents']),axis=1)\n",
    "print(dft[['tokenized_sents','sents_length']])\n",
    "print(\"the maximum sents length is\",dft['sents_length'].max())\n",
    "df4=dft['tokenized_sents'] \n",
    "df5=pd.Series.to_frame(df4) #to change series to frame'\n",
    "df5.columns=['tokenized_sents']\n",
    "# import labeled positive,negative,intensifiers and invertor lexicons\n",
    "df2=pd.read_csv('Negative.csv',names=['negative'],encoding=\"utf8\")\n",
    "df7=pd.read_csv('positive.csv',names=['positive'],encoding=\"utf8\")\n",
    "df8=pd.read_csv('Invertor.csv',names=['Invertor'],encoding=\"utf8\")\n",
    "df9=pd.read_csv('intensifierA.csv',names=['intensifier'],encoding=\"utf8\")\n",
    "columen_names=['negative','positive','Invertor','intensifierA']\n",
    "result=pd.concat([df2,df7,df8,df9],axis=1,join='outer',sort=False,names=columen_names)\n",
    "blankIndex=[''] * len(result)\n",
    "result.index=blankIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft=pd.read_csv('output.csv',delimiter=';',names=['comments'],encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_count=0\n",
    "negative_count=0\n",
    "invertor_count=0\n",
    "intensifier_count=0\n",
    "pos_emoji=0\n",
    "positive_emoji_count=0\n",
    "negative_emoji_count=0\n",
    "emoji_score=0\n",
    "reviews_len = [len(x) for x in df['comment']]\n",
    "pd.Series(reviews_len).hist()\n",
    "print(plt.show())\n",
    "print(pd.Series(reviews_len).describe())\n",
    "df10 = pd.DataFrame(columns=['polarity'])\n",
    "# most appearing positive and negative labeled Emojis \n",
    "def extract_emoji(str):\n",
    "            return ''.join(c for c in str if c in emoji.UNICODE_EMOJI)\n",
    "positive_emoji=['😂','❤','♥','😍','😘','😊','👌','💕','👏','😁','☺','♡','👍','🙏','✌','😏','😉','🙌','🙈','√', '❤️','♥️','♥️',\n",
    "'💪','😄','😃','😱','🎉','😜','🌸','💜','💙','😳','💗','☀','😎','😢','💋','😋','🙊','🎶','💞','💝',\n",
    "'💯','💛','💁','💚','😆','😝','😅','👊','😀','😚','😻','💘','👋','✋','🎊','❄','😥','😈',\n",
    "'🔝','😰','⚽','👑','😹','🍃','🎁','🐧','🎈','✊','💤','💓','💦','🙋','🎄','🎵','😛','😬','👯',\n",
    "'💎','🎂','👫','🏆','☝','😙','⛄','👅','♪','🍂','💏','🌴','👈','🌹','🙆','💰','🍻''🌞','🍁','⭐',\n",
    "'🎀','🙉','💅','🌺','🐶','🌚','🎤','👭','🎧','👆','🍸','😇','🍉','🎸','🍹','💫','📚','🌷','💝',\n",
    "'💨','🏈','💍','☔','👸','🇪','😸','🍔','👼','🐯','😵','👶','💐','↾','📖','🐒','🌍','┊','🐥','💄',\n",
    "'💸','⛔','🏀','💉','💟','😯','♦','🌙','🐟','👣','🗿','🍝','🍭','❌','🐰','💊','🚨','🍪','✧','🎆',\n",
    "'🎎','🇩','✅','🔊','👠','🌌','🍎','🐻','💇','🍊','🍒','🐭','👟','🌎','🍍','🐮','📲','🌅','🇷','🌽',\n",
    "'🍬','😺','🚀','¦🍧','🍜','🆗','🍋','➤','🏄','👧','🐏','✏','🌾','🏡','👙','⛅','🍅','📺','🐍','🇮',\n",
    "'♣🇹','🐬','🌳','💿','🔐','🐨','🌕','🔵','🍳','🚴','👰','⚓','👗','➕','💬','🔜','🍨','🍙','🍗','🍲',\n",
    "'😼','🐙','👨','🍚','🍖','♨▃','🚘']\n",
    "negative_emoji=['😭',' 😩','😒','💖','😔','😡','😴','🔫','😞','😪',' 😫','💀','😕',\n",
    "'💔','😤','😑','😠','😓','😣','😲','😿','😐' '😨','😷','👎','💩','🙅','😶','🔪','💃',\n",
    "                '👿','✂','👪','😦','🍣','🙍','💧','😾','🍥','😭']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rules for comment labeling using both labeled Emoji and lexicon\n",
    "for (index_label, row_series) in df5.iterrows():\n",
    "    list=row_series.values  #accessing the row values\n",
    "    for i in range(len(list)):\n",
    "        ll=len(list[i])\n",
    "        li=list[i]\n",
    "        print(li)              \n",
    "        xx=extract_emoji(li)\n",
    "        y=[]\n",
    "        for s in xx:\n",
    "            y.append(s)\n",
    "            ''.join(y)\n",
    "        pos_emojiexist=[]\n",
    "        neg_emojiexist=[]\n",
    "        for element in positive_emoji:\n",
    "            for word in y:\n",
    "                pos_emojiexist.append(word in element)\n",
    "                positive_emoji_count=sum(pos_emojiexist)\n",
    "        for item in negative_emoji:\n",
    "            for word in y:\n",
    "                neg_emojiexist.append(word in item)\n",
    "                negative_emoji_count=sum(neg_emojiexist)\n",
    "        if len(y)!=0:\n",
    "            emoji_score=positive_emoji_count-negative_emoji_count\n",
    "        else:\n",
    "            emoji_score=0\n",
    "        positive_res=result.positive.isin(li)\n",
    "        negative_res=result.negative.isin(li)\n",
    "        invertor_res=result.Invertor.isin(li)\n",
    "        intensifierA_res=result.intensifier.isin(li)\n",
    "        positive_count=sum(positive_res)*2\n",
    "        negative_count=sum(negative_res)*2\n",
    "        invertor_count=sum(invertor_res)\n",
    "        intensifier_count=sum(intensifierA_res)\n",
    "        sentiment_score=positive_count-negative_count\n",
    "        if invertor_count>0 and intensifier_count==0 and sentiment_score>0 and  ll>0:\n",
    "            sentiment=-(sentiment_score)\n",
    "        if invertor_count>0 and intensifier_count==0 and sentiment_score<0  and ll>0:\n",
    "            sentiment=-(sentiment_score)\n",
    "        if intensifier_count>0 and invertor_count==0 and sentiment_score>0  and ll>0:\n",
    "            sentiment=sentiment_score+3\n",
    "        if intensifier_count> 0 and invertor_count==0 and sentiment_score <0 and ll>0:\n",
    "            sentiment=sentiment_score-3\n",
    "        if invertor_count==0 and intensifier_count==0 and sentiment_score >0 and ll>0:\n",
    "            sentiment=sentiment_score\n",
    "        if invertor_count==0 and intensifier_count==0 and sentiment_score <0 and   ll>0:\n",
    "            sentiment=sentiment_score\n",
    "        if intensifier_count> 0 and invertor_count> 0 and sentiment_score>0  and ll>0:\n",
    "            sentiment=-(sentiment_score)+3\n",
    "        if intensifier_count> 0 and invertor_count> 0 and sentiment_score<0 and ll>0:\n",
    "            sentiment=-(sentiment_score)-3\n",
    "        if intensifier_count> 0 and invertor_count==0 and sentiment_score==0 and ll>0:\n",
    "             sentiment=sentiment_score\n",
    "        if sentiment_score==0 and invertor_count==0 and intensifier_count==0 and ll>0:\n",
    "             sentiment=sentiment_score\n",
    "        if sentiment>0 and emoji_score>=0:\n",
    "            print(\"positive\")\n",
    "            df10 = df10.append({'polarity': \"positive\"}, ignore_index=True)\n",
    "        if sentiment==0 and emoji_score>0:\n",
    "            print(\"positive\")\n",
    "            df10 = df10.append({'polarity': \"positive\"}, ignore_index=True)\n",
    "        if sentiment<0 and emoji_score<=0:\n",
    "            print(\"negative\")\n",
    "            df10 = df10.append({'polarity': \"negative\"}, ignore_index=True)\n",
    "        if sentiment==0 and emoji_score<0:\n",
    "            print(\"negative\")\n",
    "            df10 = df10.append({'polarity': \"negative\"}, ignore_index=True)\n",
    "        if sentiment<0 and emoji_score>0:#no change word first\n",
    "            print(\"negative\")\n",
    "            df10 = df10.append({'polarity': \"negative\"}, ignore_index=True)\n",
    "        if sentiment > 0 and emoji_score<0: #word first\n",
    "            print(\"positive\")\n",
    "            df10 = df10.append({'polarity': \"positive\"}, ignore_index=True)\n",
    "        if sentiment==0 and emoji_score==0:\n",
    "            print(\"neutral\")\n",
    "            df10 = df10.append({'polarity': \"neutral\"}, ignore_index=True)\n",
    "        if ll==0:\n",
    "            print(\"neither\")            \n",
    "df11=pd.concat([df4,df10],axis=1,join='inner',sort=False)  \n",
    "print(df11)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11['polarity'] = df11.polarity.map(lambda x: int(2) if x =='positive' else int(0) if x =='negative' else int(1) if x == 'neutral' else np.nan)\n",
    "print(df11['polarity'].value_counts())\n",
    "print(df11)\n",
    "plt.hist(df11.polarity, bins = 3, align= 'mid')\n",
    "plt.xticks(range(3), ['Negative','Neutral', 'Positive'])\n",
    "plt.xlabel('Sentiment of Reviews')\n",
    "plt.title('Distribution of Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"sentiment.csv\",\"w+\")\n",
    "fulll=[]\n",
    "for i in range(len(df11.index)):\n",
    "    x=df11.iat[i,0]\n",
    "    full = ' '.join(x)\n",
    "    f.write(full)\n",
    "    f.write('\\n')\n",
    "f.close()\n",
    "f=open(\"sentiment.csv\",\"r\")\n",
    "df=pd.read_csv('sentiment.csv',delimiter=';',names=['text'],encoding=\"utf-8\")\n",
    "all=pd.DataFrame(df)\n",
    "vectorizer=CountVectorizer()\n",
    "#Finding the bigram representation \n",
    "bigram_vectorizer=CountVectorizer(ngram_range=(1,2),max_features=24200)\n",
    "X=bigram_vectorizer.fit_transform(df['text']).toarray()\n",
    "print(X.shape)\n",
    "y=df11['polarity'].astype('int')\n",
    "print(y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 7)\n",
    "model =MultinomialNB() # use GaussianNB,BernoulliNB for other Naive Bayes models\n",
    "model.fit(X_train, y_train)\n",
    "expected = y_test\n",
    "predicted = model.predict(X_test)\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(expected, predicted))\n",
    "n=X_test.shape[0]\n",
    "error=np.sum(y_test!=predicted)\n",
    "error=error*100\n",
    "error=error/float(n)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the confusion matrix\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print(cm)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout() \n",
    "    plt.ylabel('True')  \n",
    "    plt.xlabel('Predicted')\n",
    "# plot the confusion Matrix\n",
    "cm = confusion_matrix(expected,predicted)\n",
    "plot_confusion_matrix(cm, {'negative': 0, 'neutral':1,'positive':2})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
